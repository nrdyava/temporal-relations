{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39a91e6-d41c-4059-bd23-3aaad9ff1fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvmm-filer2/users/kevin/miniconda3/envs/allinone/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "allinone_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(allinone_path)\n",
    "\n",
    "from AllInOne.datasets import ActivityNetDataset\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d76732-d88b-4a8d-b3d9-b60468602293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4ea68-de9a-4f46-a46e-be5ae7dbcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9809888-1a18-4a7a-9a84-ca228ad5fe35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(modal_ckpt, torch_device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_ckpt)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForCausalLM.from_pretrained(model_ckpt)\n",
    "    model = model.to(torch.float16)\n",
    "    model = model.to(torch_device)\n",
    "    # model = None\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ac6bf2-9a05-4563-b4f3-08371597a10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    dataset = ActivityNetDataset(\n",
    "        split='test', \n",
    "        num_frames=3, \n",
    "        image_size=224, \n",
    "        max_text_len=40, \n",
    "        masking_type='image', \n",
    "        masking_prob=0.0,\n",
    "        transform=True,\n",
    "        use_git=False,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        shuffle=False, \n",
    "        num_workers=15, \n",
    "        batch_size=1,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9191cd-4def-4276-82c4-62b49df3f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama(model, tokenizer, dataloader, torch_device):\n",
    "    \n",
    "    llama_outputs = []\n",
    "    for data in tqdm(dataloader):\n",
    "        batch = []\n",
    "        batch_outputs = []\n",
    "        for idx in range(len(data['text_1'][0])):\n",
    "            prompt = PROMPT + \\\n",
    "                    '\\nEvent 1: ' + data['text_1'][0][idx] + \\\n",
    "                    '\\nEvent 2: ' + data['text_2'][0][idx] + \\\n",
    "                    '\\n\\n' + INSTRUCTION + '\\n[/INST]'\n",
    "            batch.append(prompt)\n",
    "            batch_outputs.append({\n",
    "                'event1': data['text_1'][0][idx],\n",
    "                'event2': data['text_2'][0][idx],\n",
    "            })\n",
    "        # print(batch)\n",
    "\n",
    "        inputs = tokenizer(batch, padding=True, return_tensors=\"pt\").to(torch_device)\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=3000)\n",
    "        output = tokenizer.batch_decode(\n",
    "            generate_ids, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "\n",
    "        # print(output)\n",
    "        for idx, out in enumerate(output):\n",
    "            prompt_len = len(batch[idx])\n",
    "            answer_json_str1 = out[prompt_len:]\n",
    "            try:\n",
    "                answer_json = json.loads(answer_json_str1)\n",
    "                answer = answer_json['answer']\n",
    "                explanation = answer_json['explanation']\n",
    "            except:\n",
    "                try:\n",
    "                    last_inst_idx = out.rfind('{')\n",
    "                    answer_json_str2 = out[last_inst_idx:]\n",
    "                    answer_json = json.loads(answer_json_str2)\n",
    "                    answer = answer_json['answer']\n",
    "                    explanation = answer_json['explanation']\n",
    "                except:\n",
    "                    answer = 'unclear'\n",
    "            batch_outputs[idx]['answer'] = answer\n",
    "            batch_outputs[idx]['explanation'] = explanation\n",
    "        # print(batch_outputs)\n",
    "\n",
    "        llama_outputs += batch_outputs\n",
    "\n",
    "    with open('relationships.json', 'w') as f:\n",
    "        json.dump(llama_outputs, f, indent=4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844fa12e-cfef-48cd-897a-bd10b63c1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_subset(model, tokenizer, torch_device):\n",
    "    fname = '../data_viz/samples_200.csv'\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "   \n",
    "    # data = data[1:]\n",
    "    llama_outputs = []\n",
    "    for sample in data[1:]:\n",
    "        idx = sample[0]\n",
    "        prompt = PROMPT + \\\n",
    "                '\\nEvent 1: ' + sample[1] + \\\n",
    "                '\\nEvent 2: ' + sample[2] + \\\n",
    "                '\\n\\n' + INSTRUCTION + '\\n[/INST]'\n",
    "        batch = [prompt]\n",
    "\n",
    "        inputs = tokenizer(batch, padding=True, return_tensors=\"pt\").to(torch_device)\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=3000)\n",
    "        output = tokenizer.batch_decode(\n",
    "            generate_ids, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )[0]\n",
    "\n",
    "        prompt_len = len(batch[0])\n",
    "        answer_json_str1 = output[prompt_len:]\n",
    "        try:\n",
    "            answer_json = json.loads(answer_json_str1)\n",
    "            answer = answer_json['answer']\n",
    "            explanation = answer_json['explanation']\n",
    "        except:\n",
    "            try:\n",
    "                last_inst_idx = output.rfind('{')\n",
    "                answer_json_str2 = output[last_inst_idx:]\n",
    "                answer_json = json.loads(answer_json_str2)\n",
    "                answer = answer_json['answer']\n",
    "                explanation = answer_json['explanation']\n",
    "            except:\n",
    "                answer = 'unclear'\n",
    "                explanation = 'none'\n",
    "        sample[-1] = answer\n",
    "        sample.append(explanation)\n",
    "        # llama_outputs.append({\n",
    "        #     'idx': idx,\n",
    "        #     'answer': answer,\n",
    "        #     'explanation': explanation,\n",
    "        # })\n",
    "\n",
    "    # with open('samples+llama.json', 'w') as f:\n",
    "    #     json.dump(llama_outputs, f, indent=4)\n",
    "    with open(fname+'+llama2', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180152c-9d82-49d9-96b3-3d7a42aee7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_relationships():\n",
    "    with open('relationships.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def answer_to_binary(answer):\n",
    "        if answer == 'Yes' or answer == 'yes':\n",
    "            return 1\n",
    "        elif answer == 'No' or answer == 'no':\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    relationships = [answer_to_binary(sample['answer']) for sample in data]\n",
    "    filtered_relationships = [r for r in relationships if r is not None]\n",
    "    filtered_relationships = np.array(filtered_relationships)\n",
    "    N = len(relationships)\n",
    "    F = len(filtered_relationships)\n",
    "    good_samples = np.sum(filtered_relationships)\n",
    "    bad_samples = F - good_samples\n",
    "    unk_samples = N - F\n",
    "    print('Number of good samples:', good_samples)\n",
    "    print('Number of bad samples:', bad_samples)\n",
    "    print('Number of inconclusive samples:', unk_samples)\n",
    "    print('Percentage of good samples:', np.round(100 * good_samples / N, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdefd02-b9e7-4527-aa1e-3caa2edecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_relationships():\n",
    "    with open('relationships/relationships.json', 'r') as f:\n",
    "        relationships = json.load(f)\n",
    "        print(len(relationships))\n",
    "    has_relationship = {}\n",
    "    for r in relationships:\n",
    "        events = (r['event1'], r['event2'])\n",
    "        label = not (r['answer'] in ['no', 'No'])\n",
    "        has_relationship[events] = label\n",
    "    with open('has_relationship.pkl', 'wb') as f:\n",
    "        pk.dump(has_relationship, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8295c-46ca-4e90-8343-5d66d3145800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed6c57-32f3-493a-9742-6c1241b49eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d411f1-ae99-456a-b7b6-410833a74553",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=\"\"\"<s>[INST] <<SYS>>\n",
    "In this task, you will be given two texts which summarize two different events. \n",
    "The two events may be from the same context, but may also be unrelated or have a weak relationship.\n",
    "Your task is to determine if the two events have a likely temporal relation, meaning one procedes the other.\n",
    "\n",
    "Each problem has all of the following information:\n",
    "- A summary of the two events, each in a different line\n",
    "\n",
    "<</SYS>>\n",
    "Event 1: The man in the grey and white shirt enters the enclosed squash court picks up some of the balls and proceeds to load the squash cannon serving machines .\n",
    "Event 2: A man wearing a white and grey shirt serves in a practice squash session and another man wearing a purple shirt returns the serves in an enclosed squash court .\n",
    "\n",
    "Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\n",
    "[/INST]{\"answer\": \"Yes\", \"explanation\": \"The serving machine must be loaded with balls before the man can practice serving.\"}</s>\n",
    "<s>[INST]\n",
    "\n",
    "Event 1: Sumo wrestlers lift up legs and then crouch .\n",
    "Event 2: Sumo wrestlers eat food in the dojo .\n",
    "\n",
    "Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\n",
    "[/INST]{\"answer\": \"No\", \"explanation\": \"It is ambiguous which event comes first as neither must necessarily precede the other.\"}</s>\n",
    "<s>[INST]\"\"\"\n",
    "INSTRUCTION = \"Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99f028-dec4-46f9-be08-1343edf044ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4baa896-ec55-40dc-9b72-b123372e5c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"/dvmm-filer3a/users/hammad/llama/llama-2-13b-chat-hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2626ce9a-8b88-4047-888f-b843b7018f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb1e0b-4e2d-406f-b38f-c95e7c04e021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac85dfb7-f7d8-47ed-873e-58e6bd92dca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using test_19k.json\n"
     ]
    }
   ],
   "source": [
    "dataloader = load_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d64d33-2236-4806-8510-9b4c75e0740a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:21<00:00,  7.26s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_ckpt, torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d637b30-2617-43ac-a91f-4af02ee1930e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cca89-12dd-432f-9019-7e4029b8658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_llama_subset(model, tokenizer, torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55490900-ebd3-4099-96a6-b0aa91b48f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28c629-ac83-4e7d-b34b-12c44843acdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce81c30-2611-447d-9628-69bc5d39e90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b185b3f-f68b-4078-b410-cb43b7d7306a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7e0f6-aef4-485d-aec4-174cc805c42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ca3f8-a85f-4201-806f-24a80e6213fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b0b80-2bf1-4ce9-a53d-1880922de357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49176ba9-3d53-4f2b-848a-84df58b64e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28315e7d-5785-4896-a9a3-887591cdc092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658127f-d185-455f-916c-9242f8353e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150bc33-173d-499a-8af4-aa0a78f434e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9fa33-7d4b-43d1-b59f-74524dec39af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802938d-a7ac-47a8-9cb0-925e8b89b47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d80cc-757d-4951-892d-766b7b491405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd670c05-cd72-4cad-bfed-395b41ca0689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
