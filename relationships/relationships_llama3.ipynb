{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4085f49a-9305-4cbd-894b-2f2eb4e17e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrdyava/miniconda3/envs/tr-llama3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nallinone_path = os.path.dirname(os.getcwd())\\nsys.path.append(allinone_path)\\n\\nfrom AllInOne.datasets import ActivityNetDataset\\nfrom transformers import (AutoTokenizer,\\n                          AutoModelForCausalLM,\\n                          BitsAndBytesConfig,\\n                          pipeline)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "#from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "cache_dir = '/home/nrdyava/main/hf_home'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "\"\"\"\n",
    "allinone_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(allinone_path)\n",
    "\n",
    "from AllInOne.datasets import ActivityNetDataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline)\n",
    "\"\"\"\n",
    "#cache_dir = '/dvmm-filer3a/users/nrdyava/hf_home'\n",
    "#os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89642929-9176-4d9a-8dd1-64933b082911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf53df3-2168-4e2e-981d-141a61b49151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/nrdyava/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = \"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\"\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f36d3-5abb-48f0-ac8f-9a2243e93bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71aa3402-9a0d-4d30-ad00-b4432ee60448",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"\"\"In this task, you will be given two texts which summarize two different events. \n",
    "The two events may be from the same context, but may also be unrelated or have a weak relationship.\n",
    "Your task is to determine if the two events have a likely temporal relation, meaning one procedes the other.\n",
    "\n",
    "Each problem has all of the following information:\n",
    "- A summary of the two events, each in a different line\"\"\"\n",
    "\n",
    "user_prompt_1 = \"\"\"Event 1: The man in the grey and white shirt enters the enclosed squash court picks up some of the balls and proceeds to load the squash cannon serving machines .\n",
    "Event 2: A man wearing a white and grey shirt serves in a practice squash session and another man wearing a purple shirt returns the serves in an enclosed squash court .\n",
    "\n",
    "Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\"\"\"\n",
    "\n",
    "assistant_response_1 = \"\"\"{\"answer\": \"Yes\", \"explanation\": \"The serving machine must be loaded with balls before the man can practice serving.\"}\"\"\"\n",
    "\n",
    "user_prompt_2 = \"\"\"Event 1: Sumo wrestlers lift up legs and then crouch .\n",
    "Event 2: Sumo wrestlers eat food in the dojo .\n",
    "\n",
    "Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\"\"\"\n",
    "\n",
    "assistant_response_2 = \"\"\"{\"answer\": \"No\", \"explanation\": \"It is ambiguous which event comes first as neither must necessarily precede the other.\"}\"\"\"\n",
    "\n",
    "INSTRUCTION = \"Do the two events summarized above have a temporal relationship? Please choose between Yes and No. Please answer in json format with explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce6038-7125-4c99-9fab-31782ebfda71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29103d9f-a680-45c2-81b1-f965a1c43345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509cbc3a-3699-4b50-b349-03295c119eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama_subset(pipeline, terminators):\n",
    "    fname = '../data_viz/samples_200.csv'\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    llama_outputs = []\n",
    "\n",
    "    for sample in data[1:]:\n",
    "        idx = sample[0]\n",
    "        user_prompt_3 = 'Event 1: ' + sample[1] + '\\nEvent 2: ' + sample[2] + '\\n\\n' + INSTRUCTION\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_1},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response_1},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_2},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response_2},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_3}\n",
    "        ]\n",
    "\n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=3000,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        answer_json_str1 = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "        try:\n",
    "            answer_json = json.loads(answer_json_str1)\n",
    "            answer = answer_json['answer']\n",
    "            explanation = answer_json['explanation']\n",
    "        except:\n",
    "            try:\n",
    "                last_inst_idx = output.rfind('{')\n",
    "                answer_json_str2 = output[last_inst_idx:]\n",
    "                answer_json = json.loads(answer_json_str2)\n",
    "                answer = answer_json['answer']\n",
    "                explanation = answer_json['explanation']\n",
    "            except:\n",
    "                answer = 'unclear'\n",
    "                explanation = 'none'\n",
    "\n",
    "        sample[-1] = answer\n",
    "        sample.append(explanation)\n",
    "        # llama_outputs.append({\n",
    "        #     'idx': idx,\n",
    "        #     'answer': answer,\n",
    "        #     'explanation': explanation,\n",
    "        # })\n",
    "\n",
    "    # with open('samples+llama.json', 'w') as f:\n",
    "    #     json.dump(llama_outputs, f, indent=4)\n",
    "    with open(fname+'+llama3', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535587d-6dea-4cbf-ac57-0f291d2bb231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9d839-c94f-4c56-a337-ff56f6ab32e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd44d4e0-1382-4eb8-bb3b-5f3e0a8edaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:51<00:00, 12.94s/it]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743eb08-7ece-4444-b943-37c84e0925a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1352c-4684-4276-8a53-37cc3f496c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "run_llama_subset(pipeline, terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2056f-c6f6-45a0-be19-c2efeeca62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
