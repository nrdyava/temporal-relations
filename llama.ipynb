{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b5aa7a-1ab9-4a5d-9456-385455baac40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvmm-filer2/users/kevin/miniconda3/envs/allinone/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from AllInOne.datasets import ActivityNetDataset\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0586c-4e28-4fb5-94a8-e4d402aad055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9439c15-8f0c-4135-b299-deddf614a4b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    dataset = ActivityNetDataset(\n",
    "        split='test', \n",
    "        num_frames=3, \n",
    "        image_size=224, \n",
    "        max_text_len=40, \n",
    "        masking_type='image', \n",
    "        masking_prob=0.0,\n",
    "        transform=True,\n",
    "        use_git=False,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        shuffle=False, \n",
    "        num_workers=15, \n",
    "        batch_size=1,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8f1814-eb97-4e1e-b530-237ecf844510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(modal_ckpt, torch_device):\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_ckpt)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForCausalLM.from_pretrained(model_ckpt)\n",
    "    model = model.to(torch.float16)\n",
    "    model = model.to(torch_device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760f3c43-db79-4114-b563-9eb72c6b98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruction(comp):\n",
    "    if comp[-1] == 's':\n",
    "        comp = comp[:-1]\n",
    "    instruction = f\"Does event 1 {comp} before or after event 2? Please choose between before and after. Please answer in json format with explanation.\"\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080904a-a3c3-4011-aa1b-1156a1788c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_llama(model, tokenizer, dataloader, torch_device, filter=False):\n",
    "    \n",
    "    if filter:\n",
    "        print('filtering')\n",
    "        with open('relationships/has_relationship.pkl', 'rb') as f:\n",
    "            has_relationship = pk.load(f)\n",
    "            \n",
    "    batch_size = 1\n",
    "    llama_outputs = []\n",
    "    for data in tqdm(dataloader):\n",
    "        batch = []\n",
    "        batch_outputs = []\n",
    "        for idx in range(batch_size):\n",
    "            if filter:\n",
    "                key = (data['text_1'][idx][0], data['text_2'][idx][0])\n",
    "                if has_relationship[key] == 0:\n",
    "                    print('skipping')\n",
    "                    continue\n",
    "            prompt = PROMPT + \\\n",
    "                    '\\nEvent 1: ' + data['text_1'][idx][0] + \\\n",
    "                    '\\nEvent 2: ' + data['text_2'][idx][0] + \\\n",
    "                    '\\n\\n' + get_instruction(data['comp'][idx]) + '\\n[/INST]'\n",
    "            batch.append(prompt)\n",
    "            batch_outputs.append({\n",
    "                'event1': data['text_1'][idx][0],\n",
    "                'event2': data['text_2'][idx][0],\n",
    "                'comp': data['comp'][idx],\n",
    "                'label': data['label'][idx],\n",
    "            })\n",
    "        # for idx in range(len(data['text_1'])):\n",
    "        #     prompt = PROMPT + \\\n",
    "        #             '\\nEvent 1: ' + data['text_1'][0] + \\\n",
    "        #             '\\nEvent 2: ' + data['caption_2'] + \\\n",
    "        #             '\\n\\n' + get_instruction(data['comp']) + '\\n[/INST]'\n",
    "        #     batch.append(prompt)\n",
    "        #     batch_outputs.append({\n",
    "        #         'event1': data['text_1'][0],\n",
    "        #         'event2': data['caption_2'],\n",
    "        #         'comp': data['comp'],\n",
    "        #         'label': data['label'],\n",
    "        #     })\n",
    "        # print(batch)\n",
    "\n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(torch_device)\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=2048)\n",
    "        output = tokenizer.batch_decode(\n",
    "            generate_ids, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "\n",
    "        # print(output)\n",
    "        for idx, out in enumerate(output):\n",
    "            prompt_len = len(batch[idx])\n",
    "            answer_json_str1 = out[prompt_len:]\n",
    "            batch_outputs[idx]['raw_answer'] = answer_json_str1\n",
    "            try:\n",
    "                answer_json = json.loads(answer_json_str1)\n",
    "                answer = answer_json['answer']\n",
    "                explanation = answer_json['explanation']\n",
    "            except:\n",
    "                try:\n",
    "                    last_inst_idx = out.rfind('{')\n",
    "                    answer_json_str2 = out[last_inst_idx:]\n",
    "                    answer_json = json.loads(answer_json_str2)\n",
    "                    answer = answer_json['answer']\n",
    "                    explanation = answer_json['explanation']\n",
    "                except:\n",
    "                    answer = 'unclear'\n",
    "                    explanation = 'unknown'\n",
    "            batch_outputs[idx]['answer'] = answer\n",
    "            batch_outputs[idx]['explanation'] = explanation\n",
    "\n",
    "        llama_outputs += batch_outputs\n",
    "        # if len(llama_outputs) > 10:\n",
    "        #     break\n",
    "\n",
    "    with open('llama_predictions/llama_zeroshot_filtered.json', 'w') as f:\n",
    "        json.dump(llama_outputs, f, indent=4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37409fb-87fb-4f14-bd38-04bdac3bb9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_llama():\n",
    "    with open('./llama_predictions/llama_zeroshot_filtered.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    acc, unclear = [], []\n",
    "    for sample in data:\n",
    "        acc.append(int(sample['label'] == sample['answer']))\n",
    "        unclear.append(int(sample['answer'] == 'unclear'))\n",
    "    acc = np.array(acc)\n",
    "    unclear = np.array(unclear)\n",
    "    print('Samples:', len(acc))\n",
    "    print('Accuracy:', acc.mean())\n",
    "    print('Unclear rate:', unclear.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdef6d2-8248-4730-976c-0554c1c90fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0976857-f553-45a4-b35b-cec03d3e823d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2222b6-c199-4a26-9d10-7d2c76b179a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text zeroshot\n",
    "PROMPT=\"\"\"<s>[INST] <<SYS>>\n",
    "In this task, you will be given two texts which summarize two different events. \n",
    "Your task is to determine if whether or not the first event starts or ends before or after the second event.\n",
    "\n",
    "Each problem has all of the following information:\n",
    "- A summary of the two events, each in a different line\n",
    "- A relationship token (starts or ends) which indicates whether to compare the start or end times of the two events respectively\n",
    "\n",
    "<</SYS>>\n",
    "Event 1: He adds oil to the pan while talking to the camera . He then stir fries the chopped vegetables .\n",
    "Event 2: He is shown putting on an apron and then turning the stove top to high heat .\n",
    "\n",
    "Does event 1 start before or after event 2? Please choose between before and after. Please answer in json format with explanation.\n",
    "[/INST]{\"answer\": \"after\", \"explanation\": \"The man must first turn on the heat before being able to stir fry the vegetables.\"}</s>\n",
    "<s>[INST]\"\"\"\n",
    "# ------------------------------------------------------------------- # \n",
    "# multimodal\n",
    "# PROMPT=\"\"\"<s>[INST] <<SYS>>\n",
    "# In this task, you will be given two texts which summarize two different events. \n",
    "# Your task is to determine if whether or not the first event starts or ends before or after the second event.\n",
    "\n",
    "# Each problem has all of the following information:\n",
    "# - A summary of the two events, each in a different line\n",
    "# - A relationship token (starts or ends) which indicates whether to compare the start or end times of the two events respectively\n",
    "\n",
    "# For each sample, please choose between before and after. Please answer in json format with explanation. \n",
    "# An example of a properly formatted answer: {\"answer\": \"after\", \"explanation\": \"The man must first turn on the heat before being able to stir fry the vegetables.\"}\n",
    "\n",
    "# <</SYS>>\n",
    "# <s>[INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4fd27-8bc9-455d-ad21-e32ff73d7218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b72654-1b37-4f68-80cd-bcf0cbd2d33f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using test_19k.json\n"
     ]
    }
   ],
   "source": [
    "dataloader = load_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9e80e-a2bb-4af0-b7a0-86763068a830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3279502-8705-438e-a9cc-6ea2c76e012c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch.keys())\n",
    "    print(batch['image_1'][0].shape)\n",
    "    print(batch['image_2'][0].shape)\n",
    "    print(batch['time_stamp_1'])\n",
    "    print(batch['time_stamp_2'])\n",
    "    print(batch['video_key'])\n",
    "    print(batch['raw_index'])\n",
    "    print(batch['text_1_mask'])\n",
    "    print(batch['text_2_mask'])\n",
    "    print(batch['image_1_mask'])\n",
    "    print(batch['image_2_mask'])\n",
    "    print(batch['text_1'])\n",
    "    print(batch['text_2'])\n",
    "    print(batch['comp'])\n",
    "    print(batch['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bab857-0390-4957-86b3-695f256ee97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f045bf-2e8b-44c2-8ac1-34360361d4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87172328-7328-49bc-8002-007d82fd24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c21c6a-2cb8-4dea-9ba1-1d2d970722a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80e26b-4348-45b3-b9e6-4bd7047c79b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5a81f-c4f2-4280-af71-1a540c8813ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b9c803-6829-4857-816e-801ac8ec681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"/dvmm-filer3a/users/hammad/llama/llama-2-13b-chat-hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a936f6-1e12-4419-9179-227ef716cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb2392-de13-4db3-a53c-7607c23a0468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09612a9c-0fd1-4cd4-aaee-07381152fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_ckpt, torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a64ac-40bb-4ee3-a9dd-9c5a555625c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de420aa-2722-4449-8c12-e39421486242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f435e9-3ee7-4679-8286-36f8e643cdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_llama(model, tokenizer, dataloader, torch_device, filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48e7e3-7542-4674-a5f2-fd0df6fb7346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef30280-8171-4022-b044-a999573ab149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50378726-2ea6-4f94-8262-a9f8884219fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7015ee-2655-4c02-8e23-e77bf71fbc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 14028\n",
      "Accuracy: 0.5290846877673225\n",
      "Unclear rate: 0.021599657827202738\n"
     ]
    }
   ],
   "source": [
    "eval_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cfec4-56fe-430b-899e-ef0f1e0715b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924365b-1844-44bf-ba89-f836d8d500db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73e086-a820-40b5-bf61-c6986f0c2189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719178ed-5165-476e-8065-903e038af6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03755f7-3def-4cbd-b1bd-219e9d83590a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45736b93-7448-4048-af24-7e78989edaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c13980-88a4-4dfe-9c85-df0d147845c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfa4e1-eeed-4239-8e45-23fd96c24d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee115769-b096-45e5-89dd-e881dba666e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee855777-35ef-4a8f-92b5-e5cdefcd5e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
